{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sweetviz as sv\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "import smogn\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "SEED=42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR EVALUATING MODEL PERFORMANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, y_test, y_pred, y_train, X_train, X_test):\n",
    "    print('MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('MSE: ', mean_squared_error(y_test, y_pred)) \n",
    "    print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))\n",
    "    print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR PLOTTING y_pred VS y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_test, y_pred):\n",
    "    results = pd.DataFrame(zip(y_test, y_pred, y_test - y_pred), columns = ['y_test', 'y_pred', 'error'])\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    x=np.linspace(0,45,45)\n",
    "    plt.plot(results['y_test'], results['y_pred'], 'b.')\n",
    "    plt.plot(x, x, 'r-')\n",
    "    plt.xlim(0,45)\n",
    "    plt.ylim(0,45)\n",
    "    plt.title(\"Results\", fontsize=16)\n",
    "    plt.xlabel(\"Real\", fontsize=14)\n",
    "    plt.ylabel(\"Predicted\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING FEATURE IMPORTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_excel('majus29_ALL_SEER.xlsx', sheet_name='ord_cols', header=0)\n",
    "\n",
    "# converting HIST_TYPE because it has been coded as integer\n",
    "convert_dict = {'Histologic Type ICD-O-3': object}\n",
    "df = df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(df)\n",
    "\n",
    "data_categorical = df[categorical_columns]\n",
    "df_non_cat = df.drop(categorical_columns, axis=1)\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "df_encoded = pd.DataFrame(encoder.fit_transform(data_categorical.astype(str)))\n",
    "df_encoded.columns = categorical_columns\n",
    "df_enc = pd.concat([df_encoded, df_non_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_enc['Survival months']\n",
    "X = df_enc.drop(['Survival months'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Fitting model to train data\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "importances_df = pd.DataFrame({\"Feature_names\" : rfr.feature_names_in_, \n",
    "                               \"Importances\" : rfr.feature_importances_})\n",
    "importances_df = importances_df[importances_df['Importances'] >= 0.01]\n",
    "\n",
    "g = sns.barplot(data=importances_df, \n",
    "                x=\"Importances\", \n",
    "                y=\"Feature_names\",\n",
    "                palette='mako',\n",
    "                order=importances_df.sort_values('Importances', ascending=False).Feature_names)\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# setting x ticks as empty\n",
    "g.set(xticks=[])\n",
    "g.set_title(\"Feature importances\", fontsize=14)\n",
    "for value in g.containers:\n",
    "    g.bar_label(value, padding=2)# Obtaining feature importances\n",
    "\n",
    "g.figure.set_size_inches(6,6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('majus29_ALL_SEER.xlsx', sheet_name='ord_cols', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TUMOR_SIZE_mm'] = df['TUMOR_SIZE_cm_earlier'].fillna(df['TUMOR_SIZE_cm_later'])\n",
    "df = df.drop(['TUMOR_SIZE_cm_earlier', 'TUMOR_SIZE_cm_later'], axis=1)\n",
    "\n",
    "inc_dict = {'$75,000+':0.1, '$70,000 - $74,999': 0.2, '$65,000 - $69,999': 0.3, '$60,000 - $64,999':0.4, '$55,000 - $59,999':0.5, '$45,000 - $49,999':0.6, '$40,000 - $44,999':0.7, '$50,000 - $54,999':0.8, '$35,000 - $39,999':0.9, '< $35,000':1, 'Unknown/missing/no match/Not 1990-2018': np.NaN}\n",
    "df['MEDIAN_INCOME_dollars'] = df['MEDIAN_INCOME_dollars'].map(inc_dict)\n",
    "\n",
    "gr_dict = {'Moderately differentiated; Grade II':0.3, 'Unknown': np.NaN, 'Poorly differentiated; Grade III': 0.6, 'Well differentiated; Grade I': 0, 'Undifferentiated; anaplastic; Grade IV':1, 'Blank(s)':np.NaN}\n",
    "df['GRADE'] = df['GRADE'].map(gr_dict)\n",
    "\n",
    "st_dict = {'Regional': 0.5,'Localized':0, 'Unknown/unstaged': np.NaN, 'Distant': 1, 'Blank(s)':np.NaN}\n",
    "df['STAGE'] = df['STAGE'].map(st_dict)\n",
    "\n",
    "df[['HISTOLOGIC_TYPE_ICD_O_3', 'PRIMARY_SITE']] = df[['HISTOLOGIC_TYPE_ICD_O_3', 'PRIMARY_SITE']].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['SURVIVAL_TIME_months']\n",
    "X = df.drop(['SURVIVAL_TIME_months'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "HGBR_model = HistGradientBoostingRegressor(random_state = SEED)\n",
    "HGBR_model.fit(X_train, y_train)\n",
    "y_pred = HGBR_model.predict(X_test)\n",
    "evaluate_model(HGBR_model, y_test, y_pred, y_train, X_train, X_test)\n",
    "plot_results(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = IterativeImputer(estimator=linear_model.BayesianRidge(),n_nearest_features=None, imputation_order='ascending', random_state=SEED)\n",
    "imp_mean.fit(df)\n",
    "df = pd.DataFrame(imp_mean.transform(df))\n",
    "df.columns = cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCALING NUMERICAL COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_age = StandardScaler()\n",
    "df['AGE_AT_DIAGNOSIS_years'] = sc_age.fit_transform(df['AGE_AT_DIAGNOSIS_years'].values.reshape(-1,1))\n",
    "\n",
    "sc_size = StandardScaler()\n",
    "df['TUMOR_SIZE_mm'] = sc_size.fit_transform(df['TUMOR_SIZE_mm'].values.reshape(-1,1))\n",
    "\n",
    "sc_hist = StandardScaler()\n",
    "df['HISTOLOGIC_TYPE_ICD_O_3'] = sc_hist.fit_transform(df['HISTOLOGIC_TYPE_ICD_O_3'].values.reshape(-1,1))\n",
    "\n",
    "sc_site = StandardScaler()\n",
    "df['PRIMARY_SITE'] = sc_site.fit_transform(df['PRIMARY_SITE'].values.reshape(-1,1))\n",
    "\n",
    "sc_mal = StandardScaler()\n",
    "df['TOTAL_NUM_OF_MALIGNANT_TUMORS'] = sc_mal.fit_transform(df['TOTAL_NUM_OF_MALIGNANT_TUMORS'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df, x=\"SURVIVAL_TIME_months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['SURVIVAL_TIME_months']\n",
    "X = df.drop(['SURVIVAL_TIME_months'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR_model = RandomForestRegressor(random_state = SEED)\n",
    "RFR_model.fit(X_train, y_train)\n",
    "y_pred = RFR_model.predict(X_test)\n",
    "evaluate_model(RFR_model, y_test, y_pred, y_train, X_train, X_test)\n",
    "plot_results(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_mtrx = [\n",
    "    [0,1,0],\n",
    "    [2,0,0],\n",
    "    [60,0.8,0],\n",
    "    [100,0.95,0],\n",
    "    [150,1,0]\n",
    "]\n",
    "\n",
    "df_smogn = smogn.smoter(\n",
    "    \n",
    "    ## main arguments\n",
    "    data = df,           ## pandas dataframe\n",
    "    y = 'SURVIVAL_TIME_months',          ## string ('header name')\n",
    "    k = 8,                    ## positive integer (k < n)\n",
    "    pert = 0.4,              ## real number (0 < R < 1)\n",
    "    samp_method = 'extreme',  ## string ('balance' or 'extreme')\n",
    "    drop_na_col = True,       ## boolean (True or False)\n",
    "    drop_na_row = True,       ## boolean (True or False)\n",
    "    replace = False,          ## boolean (True or False)\n",
    "\n",
    "    ## phi relevance arguments\n",
    "    rel_thres = 0.6,         ## real number (0 < R < 1)\n",
    "    rel_method = 'manual',    ## string ('auto' or 'manual')\n",
    "    # rel_xtrm_type = 'both', ## unused (rel_method = 'manual')\n",
    "    # rel_coef = 1.50,        ## unused (rel_method = 'manual')\n",
    "    rel_ctrl_pts_rg = rg_mtrx ## 2d array (format: [x, y])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [None, 0.5]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 40, num = 2)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=SEED)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=SEED, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_smogn['SURVIVAL_TIME_months']\n",
    "X = df_smogn.drop(['SURVIVAL_TIME_months'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "RFR_model = RandomForestRegressor(n_estimators = 200, min_samples_split = 5, min_samples_leaf = 1, max_features = 0.5, max_depth = None, bootstrap = False, random_state = SEED)\n",
    "RFR_model.fit(X_train, y_train)\n",
    "y_pred = RFR_model.predict(X_test)\n",
    "evaluate_model(RFR_model, y_test, y_pred, y_train, X_train, X_test)\n",
    "plot_results(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_model = xgb.XGBRegressor(random_state=SEED)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "y_pred = XGB_model.predict(X_test)\n",
    "evaluate_model(XGB_model, y_test, y_pred, y_train, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter Optimization\n",
    "n_estimators = [100, 500, 900]\n",
    "max_depth = [2, 3, 5]\n",
    "booster=['gbtree']\n",
    "learning_rate=[0.05,0.1,0.15]\n",
    "min_child_weight=[1,2]\n",
    "base_score=[0.5,0.75,1]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }\n",
    "\n",
    "# Set up the random search with 4-fold cross validation\n",
    "random_cv = RandomizedSearchCV(estimator=XGB_model, param_distributions=hyperparameter_grid, cv=3, n_iter=50, scoring = 'neg_mean_absolute_error', n_jobs = 4, verbose = 5, return_train_score = True, random_state=SEED)\n",
    "\n",
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_model = xgb.XGBRegressor(base_score=0.75, booster='gbtree', callbacks=None,\n",
    "             colsample_bylevel=None, colsample_bynode=None,\n",
    "             colsample_bytree=None, early_stopping_rounds=None,\n",
    "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
    "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
    "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
    "             min_child_weight=1, monotone_constraints=None,\n",
    "             n_estimators=900, n_jobs=None, num_parallel_tree=None,\n",
    "             predictor=None, random_state=SEED)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "y_pred_XGB = XGB_model.predict(X_test)\n",
    "evaluate_model(XGB_model, y_test, y_pred_XGB, y_train, X_train, X_test)\n",
    "plot_results(y_test, y_pred_XGB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split = 0.3, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'Weights-497--4.10621.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NN = np.reshape(NN_model.predict(X_test), (2434,))\n",
    "print('MAE: ', mean_absolute_error(y_test, y_pred_NN))\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred_NN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
